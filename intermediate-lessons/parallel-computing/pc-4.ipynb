{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intermediate Parallel Computing\n",
    "### Segment 3 of 5\n",
    "\n",
    "### PySpark SQL I: Fundamentals\n",
    "\n",
    "### In this segment we will answer:\n",
    "* What are the fundamental functions used in pySpark SQL?\n",
    "* What are the fundamental data structures in pySpark?\n",
    "* How to create RDDs and do simple operations on them?\n",
    "\n",
    "\n",
    "*Lesson Developer: Mohsen Ahmadkhani, ahmad178@umn.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "<a href=\"#/slide-2-0\" class=\"navigate-right\" style=\"background-color:blue;color:white;padding:8px;margin:2px;font-weight:bold;\">Continue with the lesson</a>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "<font size=\"+1\">\n",
    "\n",
    "By continuing with this lesson you are granting your permission to take part in this research study for the Hour of Cyberinfrastructure: Developing Cyber Literacy for GIScience project. In this study, you will be learning about cyberinfrastructure and related concepts using a web-based platform that will take approximately one hour per lesson. Participation in this study is voluntary.\n",
    "\n",
    "Participants in this research must be 18 years or older. If you are under the age of 18 then please exit this webpage or navigate to another website such as the Hour of Code at https://hourofcode.com, which is designed for K-12 students.\n",
    "\n",
    "If you are not interested in participating please exit the browser or navigate to this website: http://www.umn.edu. Your participation is voluntary and you are free to stop the lesson at any time.\n",
    "\n",
    "For the full description please navigate to this website: <a href=\"../../gateway-lesson/gateway/gateway-1.ipynb\">Gateway Lesson Research Study Permission</a>.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <style>\n",
    "        .output_prompt{opacity:0;}\n",
    "    </style>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL\n",
    "\n",
    "\n",
    "In the <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fintermediate-lessons%2Fgeospatial-data%2FWelcome.ipynb&branch=master\">intermediate lesson on Geospatial Data</a>, we saw how to use `SELECT`, `FROM`, and `WHERE` clauses to query our data. In this segment we will be introducing Spark SQL that is a data querying tool similar to regular databases but different!! Regular databases like PostgreSQL, Oracle, and others have two broad responsibilities of **(long-term) storing** and **processing** (i.e., querying and analyzing) data. \n",
    "\n",
    "**Spark SQL**, however, is a framework that is mainly concerned with **processing** large datasets and **not with long term** storage. Spark SQL makes data querying parallelized and therefore faster and more efficient especially for large datasets. \n",
    "\n",
    "In this segment, we will go through the fundamental definitions and functions in pySpark SQL. First, let's create a spark context in the next slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName(\"hourofci\").setMaster(\"local[4]\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resilient Distributed Dataset (RDD)\n",
    "Spark uses RDDs as its fundamental data structure. RDDs are objects that contain data and values. We don't want to confuse you further with the details of this concept, but it's important to note that data stored as RDD is **partitioned** and distributed across the nodes of the cluster ready for parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating an RDD Object\n",
    "\n",
    "The `SparkContext` class provides the following two methods for creating RDD objects from the collection of data in different formats. \n",
    "* `parallelize()`\n",
    "* `textFile()`\n",
    "\n",
    "We'll make example of each in the upcoming slides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `parallelize()`\n",
    "\n",
    "This method creates RDD files from python's collection data of different types such as tuple, list, dictionary,  set, and pandas and numpy arrays.  \n",
    "\n",
    "In the cell below, we create the RDD variables from a list variable containing seven elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sample_list = [1, 2, [3, 4], 5, 'cat', 0, 'dog']\n",
    "rdd_list = sc.parallelize(sample_list)\n",
    "\n",
    "rdd_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `collect()`\n",
    "Ok, so far, we created an RDD file from a list and a pandas array. If you print out an RDD variable it shows `ParallelCollectionRDD[<some identifier>]`. This is how Spark understands it, but what if we want to see the actual values stored in this variable?<br>\n",
    "Here is where `collect()` method comes handy! <br>\n",
    "In the next cell we use `collect()` method to see the values inside the RDD variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The values in the created RDD variable are exactly the same as the original list variable. However, this time, these seven elements have been partitioned into the number of partitions we set in the beginning using `setMaster` function which in this case is four. \n",
    "\n",
    "Here is how we can get the number of partitions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ok, it is confirmed that the `rdd_list` variable is partitioned into four parts. \n",
    "\n",
    "If you are curious what each partition contains, use `glom()` method as follow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark does it's best to distribute the data evenly into the number of partitions. \n",
    "\n",
    "Since there are *seven* elements in this RDD variable and we have set *four* partitions, three partitions get two elements each and the last partition gets the remainder which is a single element. \n",
    "\n",
    "Next, we will use a films sample dataset to create an RDD from CSV. Please note that we slightly modified it's <a href=\"https://perso.telecom-paristech.fr/eagan/class/igr204/data/film.csv\">original file</a> for pedagogical purposes. \n",
    "\n",
    "This sample dataset has 10 columns and 1658 rows holding some information regarding movies and their properties like title, director and so on. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create an RDD file from CSV\n",
    "We can load a CSV file as an RDD dataset using `textFile()` function. When we load a file using this function, Spark itself decides the best number of partitions based on the file's size. However, if we want to specify the number of partitions ourselves (for perfomance reasons) we can do so by passing the `minPartitions` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rddFromCSV = sc.textFile(\"supplementary/films.csv\", minPartitions = 4)\n",
    "\n",
    "rddFromCSV.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Please note that the `textFile()` function reads each row of the CSV file as a single string element and returns an RDD of strings. So, according to our data analysis purposes we usually need to use `map` function to generate our suitable data structure. \n",
    "\n",
    "So, let's see how `map` works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `map` function\n",
    "`map` function is not specifically a Spark function. It is a python built-in function that performs an operation on the elements of an iterable item without using a `for` loop. This function gets an input function and an iterable item and performs the function on each element of the interable variable. \n",
    "\n",
    "In the next cell we use the map function to square the elements of a list variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "simple_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "def square(el):\n",
    "     return el ** 2\n",
    "\n",
    "res = map(square, simple_list)\n",
    "list(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `map` Function in PySpark\n",
    "\n",
    "PySpark has implemented the `map` function as a method for each RDD variable that gets a function as input and applies it on all elements of the RDD file. \n",
    "\n",
    "In the cell belwo, we define a splitter function that splits the input text element into a list of multiple words separated by `,`. Then we apply it to all elements of our films' RDD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def splitter(el):\n",
    "    return el.split(\",\")\n",
    "\n",
    "rddFromCSV.map(splitter).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filtering Data \n",
    "\n",
    "Often we want to make data selections and fetch elements that satisfy a given condition. Python also have a built-in function for this purpose named `filter`. Similar to `map`, `filter` gets a discriminator function and returns all elements of an input iterable item that pass the discriminator function. \n",
    "\n",
    "The discriminator function tests a given condition on each element and returns True or False. \n",
    "\n",
    "To clear this up, see an example in the next slide. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def range_tester(el):\n",
    "    if el > 10:\n",
    "        return True\n",
    "\n",
    "\n",
    "num_list = [1, 23, 0, 3, 5, 11]\n",
    "    \n",
    "list(filter(range_tester, num_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `filter` Function in PySpark\n",
    "\n",
    "Similarly, `filter` is also implemented in PySpark. In the example below, we use our films RDD to illustrate this function by filtering all movies that have the word \"Life\" in them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def life_finder(el):\n",
    "    if 'Life' in el:\n",
    "        return True\n",
    "\n",
    "rddFromCSV.filter(life_finder).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lambda Function\n",
    "\n",
    "One last thing to learn in this segment is the `lambda` function that is used in PySpark very often. \n",
    "`lambda` function is a one-line short form of functions in python that can have only one expression. See an example below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = lambda a: a**2\n",
    "\n",
    "square(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lambda Function in PySpark\n",
    "In pySpark, we `map` a one-line `lambda` function to all of our RDD elements. Below is the same example we saw to split the rows into multiple words seperated by `,`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddFromCSV.map(lambda el: el.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now you are acquainted with some basic functions of pySpark SQL. In the next segment, we will learn about Spark DataFrames and applications of pySpark SQL in executing SQL codes for non-spatial data. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" \n",
    "href=\"pc-5.ipynb\">Click here to go to the next notebook.</a></font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='hciheader'></div><div class='hcifooter'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
