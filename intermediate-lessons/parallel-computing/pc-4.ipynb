{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intermediate Parallel Computing\n",
    "### Segment 3 of 6\n",
    "\n",
    "### PySpark SQL I: Fundamentals\n",
    "\n",
    "### In this segment we will answer:\n",
    "* What are the fundamental functions used in pySpark SQL?\n",
    "* What are the fundamental data structures in pySpark?\n",
    "* How to create RDDs and do simple operations on them?\n",
    "\n",
    "\n",
    "*Lesson Developer: Mohsen Ahmadkhani, ahmad178@umn.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "<a href=\"#/slide-2-0\" class=\"navigate-right\" style=\"background-color:blue;color:white;padding:8px;margin:2px;font-weight:bold;\">Continue with the lesson</a>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "<font size=\"+1\">\n",
    "\n",
    "By continuing with this lesson you are granting your permission to take part in this research study for the Hour of Cyberinfrastructure: Developing Cyber Literacy for GIScience project. In this study, you will be learning about cyberinfrastructure and related concepts using a web-based platform that will take approximately one hour per lesson. Participation in this study is voluntary.\n",
    "\n",
    "Participants in this research must be 18 years or older. If you are under the age of 18 then please exit this webpage or navigate to another website such as the Hour of Code at https://hourofcode.com, which is designed for K-12 students.\n",
    "\n",
    "If you are not interested in participating please exit the browser or navigate to this website: http://www.umn.edu. Your participation is voluntary and you are free to stop the lesson at any time.\n",
    "\n",
    "For the full description please navigate to this website: <a href=\"../../gateway-lesson/gateway/gateway-1.ipynb\">Gateway Lesson Research Study Permission</a>.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <style>\n",
    "        .output_prompt{opacity:0;}\n",
    "    </style>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL as a DataBase Management System (DBMS)\n",
    "\n",
    "\n",
    "Recall the <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fintermediate-lessons%2Fgeospatial-data%2FWelcome.ipynb&branch=master\">intermediate lesson on Geospatial Data</a> where we mentioned that a DBMS is an interface that bridges users and the physical databases through a unified Structured Query Language (SQL). Spark SQL also uses this unified language for data query. SQL is a software module that allows you to search, filter, analyze, and generally query your database. \n",
    "\n",
    "Spark SQL works like a DBMS execpt it enables data partitioning and therefore parallelizes the querying process. We discussed the SQL language in the <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fintermediate-lessons%2Fgeospatial-data%2FWelcome.ipynb&branch=master\">intermediate lesson on Geospatial Data</a>, but don't wory if you have not completed it yet. We will provide an introduction to SQL in the next few slides. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SQL and Relational Databases\n",
    "\n",
    "**Structured Query Language (SQL)** is a programming language developed to interact and communicate with a relational database. A **relational database** is a database where each dataset is stored in the form of a `Table` (a.k.a relation). Each `Table` is made of rows (a.k.a `records`) and columns (a.k.a `fields`). Each record is an instance of an entity and each column records a specific attribute of that entity. Figure below shows the first 5 records of a table named `US_States`. \n",
    "\n",
    "We will use table `US_States` to illustrate SQL in a few slides. This table contains 56 records representing US states. There are 5 fields in this table that keep information like index (`gid`), FIPS code (`statefp`), unique GeoID (`geoid`), abbreviation (`stusps`), and `name` for each US state. This dataset is downloaded from the <a href='https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html'> US Census Bureau</a> data center.\n",
    "\n",
    "<img src = \"supplementary/us_state.png\" width = \"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SQL Basic Keywords\n",
    "\n",
    "SQL language reads very similar to plain English and contains three main statements namely `SELECT`, `FROM`, and `WHERE`. Next we will demonstrate these keywords by making sample statements using our `US_State` table. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **`SELECT` statement:**\n",
    "- Tells the DBMS that which **fields** to select/filter. \n",
    "\n",
    "The general **syntax*** for **select statement** is \n",
    "\n",
    ">```mysql\n",
    "SELECT field1,field2,...,fieldN \n",
    "\n",
    "\n",
    "Syntax: `SELECT` followed by a **comma separated** list of fields. <br>\n",
    "**Note:** we use asterisk symbol (`*`) to indicate *all fields* instead of listing them all. \n",
    "\n",
    "***Syntax** of a programming language is a set of punctuation rules that defines what the combination of symbols and characters means to the computer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **`FROM` statement:**\n",
    "- Tells the DBMS that which table to look for. \n",
    "\n",
    "Here we select all fields from the `US_State` table. \n",
    "\n",
    ">```mysql\n",
    "SELECT * \n",
    "FROM US_State\n",
    "\n",
    "\n",
    "<img src = \"supplementary/us_state2.png\" width = \"700px\">\n",
    " \n",
    "We just `selected` all fields `from` the `US_State` table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `WHERE` Statement\n",
    "- Tells the DBMS to select the records that meet a certain **condition**. \n",
    "\n",
    "We usually use comparison operators such as `=`, `>`, `<`, `>=`, `<=` , and `!=` (i.e., not equal to) to make WHERE-clauses. \n",
    "\n",
    "We can define multiple conditions using `AND` (if we want both conditions to be met at the same time) and `OR` (if we want at least one of the conditions to be met) logical operators. \n",
    "\n",
    "Here is the final syntax of a query made of the three statements: \n",
    ">```mysql\n",
    "SELECT column1, column2, ..columnN\n",
    "FROM table_name\n",
    "WHERE condition1 AND condition2\n",
    "\n",
    "\n",
    "Let's look at a US_State example! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An SQL Example\n",
    "\n",
    "**Problem:** Select the name, abbreviation, and the FIPS code of the state of Minnesota from the US_State table.\n",
    "\n",
    ">```mysql\n",
    "SELECT name, stusps, statefp\n",
    "FROM US_State\n",
    "WHERE name = 'Minnesota'\n",
    "\n",
    "And here is what we get: \n",
    "<center><img src = \"supplementary/mnquery.png\" width = \"600px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting Back to Spark SQL\n",
    "\n",
    "Now that you are familiar with the basics of SQL language, let's continue exploring Spark SQL framework.\n",
    "\n",
    "Later in this segment, we will go through the fundamental definitions and functions of PySpark SQL. \n",
    "\n",
    "First, we need to create a **Spark Context**. `Spark Context` is the entry point or gateway to PySpark framework. Although Spark Context is not exactly the same as the Spark Session, it's safe enough to consider them equivalent at this stage. \n",
    "\n",
    "In the next slide we create a Spark Context and store it in a variable named `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName(\"hourofci\").setMaster(\"local[4]\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resilient Distributed Dataset (RDD)\n",
    "Spark uses RDDs as its fundamental data structure. RDDs are objects that contain data and values. We don't want to confuse you further with the details of this concept, but it's important to note that data stored as RDD is **partitioned** and distributed across the nodes of the cluster ready for parallelization. Figure below shows how an original dataset is transformed/partitioned to an RDD. \n",
    "\n",
    "<img src = \"supplementary/rdd.png\" width = \"700px\">\n",
    "\n",
    "In this figure `P` stands for `Partition`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating an RDD Object\n",
    "\n",
    "The `SparkContext` class provides the following two methods for creating RDD objects from the collection of data in different formats. \n",
    "* `parallelize()`\n",
    "* `textFile()`\n",
    "\n",
    "We'll make example of each in the upcoming slides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `parallelize()`\n",
    "\n",
    "This method creates RDD files from python's collection data of different types such as tuple, list, dictionary,  set, and pandas and numpy arrays.  \n",
    "\n",
    "In the next slide, we create an `RDD` variable from a `list` variable containing seven elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sample_list = [1, 2, 'mouse', 5, 'cat', 0, 'dog']\n",
    "rdd_list = sc.parallelize(sample_list)\n",
    "\n",
    "rdd_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `collect()`\n",
    "Ok, so far, we've created an RDD file from a list. If you print out an RDD variable it shows `ParallelCollectionRDD[<some identifier>]`. This is how Spark understands it, but what if we want to see the actual values stored in this variable?<br>\n",
    "Here is where `collect()` method comes handy! <br>\n",
    "In the cell below we use `collect()` method to see the values inside the created RDD variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The values in the created RDD variable are exactly the same as the original list variable. However, this time, these seven elements have been partitioned into the number of partitions we set in the beginning using `setMaster` function which in this case is **4**. \n",
    "\n",
    "Here is how we can get the number of partitions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ok, it is confirmed that the `rdd_list` variable is partitioned into four parts. \n",
    "\n",
    "If you are curious what each partition contains, use `glom()` method as follow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark does its best to split the data evenly into the number of partitions. \n",
    "\n",
    "Since there are *seven* elements in this RDD and we have set *four* partitions, three partitions get two elements each and the last partition gets the remainder which is a single element. \n",
    "\n",
    "Next, we will create an RDD from a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CSV Files\n",
    "CSV (comma-separated values) is a popular file format that stores tables through a specific syntax. \n",
    "\n",
    "For example consider the US_State table:<br>\n",
    "<center><img src = \"supplementary/us_state2.png\" width = \"500px\"></center><br>\n",
    "This table can be stored as a CSV file like below:\n",
    "\n",
    ">```csv\n",
    "gid, statefp, geoid, stusps, name\n",
    "1, \"28\", \"28\", \"MS\", \"Mississippi\"\n",
    "2, \"37\", \"37\", \"NC\", \"North Carolina\"\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating an RDD From a CSV\n",
    "\n",
    "We will use a `films` sample CSV dataset to create an RDD. Please note that we slightly modified its <a href=\"https://perso.telecom-paristech.fr/eagan/class/igr204/data/film.csv\">original file</a> for pedagogical purposes. \n",
    "\n",
    "This sample dataset has 10 columns and 1658 rows holding some information regarding movies and their properties like title, director and so on. \n",
    "\n",
    "\n",
    "We can load a CSV file as an RDD dataset using `textFile()` function. When we load a file using this function, Spark itself decides the best number of partitions based on the file's size. However, if we want to specify the number of partitions ourselves (for perfomance reasons) we can do so by passing the `minPartitions` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rddFromCSV = sc.textFile(\"supplementary/films.csv\", minPartitions = 4)\n",
    "\n",
    "rddFromCSV.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Please note that the `textFile()` function reads each row of the CSV file as a single `string` element and returns an RDD of strings. So, according to our data analysis purposes we usually need to use `map` function to generate our suitable data structure. \n",
    "\n",
    "So, let's see how `map` works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `map` Function\n",
    "`map` function is not specifically a Spark function. It is a python built-in function that performs an operation on the elements of an iterable item without using a `for` loop. `map` gets an input **function** and an **iterable item** and performs the function on each element of the interable variable. \n",
    "\n",
    "In the next slide we use the map function to square the elements of a list variable. \n",
    "\n",
    "Please note that the output of a map function is a **`map object`** that we need to convert to a `list` to visualize it at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "simple_list = [1, 2, 3, 4, 5]     # input iterable variable\n",
    "\n",
    "def square(el):                   # define a function \"square\" that gets a number and returns its square\n",
    "     return el ** 2\n",
    "\n",
    "res = map(square, simple_list)    # apply the function \"square\" to all elements of \"simple_list\"\n",
    "list(res)                         # convert the result to a list for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `map` Function in PySpark\n",
    "\n",
    "PySpark has implemented the `map` function as a method for each RDD variable that gets a function as input and applies it to all elements of the RDD file. \n",
    "\n",
    "In the next slide, we define a splitter function that splits an input text element (e.g., a film title) into a list of multiple words separated by `,`. Then we apply it to all elements of our films' RDD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def splitter(el):\n",
    "    return el.split(\",\")\n",
    "\n",
    "rddFromCSV.map(splitter).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filtering Data \n",
    "\n",
    "Often we want to make data selections and fetch elements that satisfy a given condition. Python also has a built-in function for this purpose named `filter`. Similar to `map`, `filter` gets a discriminator function and returns all elements of an input iterable item that pass the discriminator function. \n",
    "\n",
    "The discriminator function tests a given condition on each element and returns `True` or `False`. \n",
    "\n",
    "To clear this up, see an example in the next slide. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def range_tester(el):          # gets a value and returns True if it is larger ...\n",
    "    if el > 10:                                       #...than 10 otherwise False\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "num_list = [1, 23, 0, 3, 5, 11]\n",
    "    \n",
    "filtered = filter(range_tester, num_list) # apply range_tester function to elements of num_list\n",
    "list(filtered)        # convert the result to a list to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `filter` Function in PySpark\n",
    "\n",
    "Similarly, `filter` is also implemented in PySpark. In the example below, we use our films RDD to illustrate this function by filtering all movies that have the word <i>\"Life\"</i> in them.\n",
    "\n",
    "<i>*Remember this example. In the next segment we will see how we can do the same thing with Spark SQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def life_finder(el):\n",
    "    if 'Life' in el:\n",
    "        return True\n",
    "\n",
    "rddFromCSV.filter(life_finder).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now you are acquainted with some basic functions of pySpark SQL. In the next segment, we will learn about Spark DataFrames and applications of pySpark SQL in executing SQL codes for non-spatial data. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" \n",
    "href=\"pc-5.ipynb\">Click here to go to the next notebook.</a></font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='hciheader'></div><div class='hcifooter'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
