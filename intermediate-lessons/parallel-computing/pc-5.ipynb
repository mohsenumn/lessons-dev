{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intermediate Parallel Computing\n",
    "\n",
    "### Segment 4 of 5\n",
    "\n",
    "### PySpark SQL II: Non-Spatial SQL Query\n",
    "\n",
    "### In this segment we will learn:\n",
    "* Spark SQL and spark dataframes.\n",
    "* Querying non-spatial data with PySpark SQL.\n",
    "\n",
    "\n",
    "*Lesson Developer: Mohsen Ahmadkhani, ahmad178@umn.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "<a href=\"#/slide-2-0\" class=\"navigate-right\" style=\"background-color:blue;color:white;padding:8px;margin:2px;font-weight:bold;\">Continue with the lesson</a>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "<font size=\"+1\">\n",
    "\n",
    "By continuing with this lesson you are granting your permission to take part in this research study for the Hour of Cyberinfrastructure: Developing Cyber Literacy for GIScience project. In this study, you will be learning about cyberinfrastructure and related concepts using a web-based platform that will take approximately one hour per lesson. Participation in this study is voluntary.\n",
    "\n",
    "Participants in this research must be 18 years or older. If you are under the age of 18 then please exit this webpage or navigate to another website such as the Hour of Code at https://hourofcode.com, which is designed for K-12 students.\n",
    "\n",
    "If you are not interested in participating please exit the browser or navigate to this website: http://www.umn.edu. Your participation is voluntary and you are free to stop the lesson at any time.\n",
    "\n",
    "For the full description please navigate to this website: <a href=\"../../gateway-lesson/gateway/gateway-1.ipynb\">Gateway Lesson Research Study Permission</a>.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <style>\n",
    "        .output_prompt{opacity:0;}\n",
    "    </style>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's Get Our Hands Dirty!\n",
    "\n",
    "Great job thus far! <br>\n",
    "It's time to CODE!  \n",
    "\n",
    "In this segment, we will introduce Spark DataFrames and see how to query them in SQL language. \n",
    "\n",
    "Currently, you are in 'Presentation mode.' For the rest of this lesson, we will exit the RISE Presentation mode for convenient coding. Work your way down the notebooks, running code cells as you go. \n",
    "\n",
    "Click on the **X** in the upper left hand corner to exit RISE Presentation mode and see the regular Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## First Step\n",
    "\n",
    "Let's import Spark modules and create a spark context first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName(\"hourofci\").setMaster(\"local[*]\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Spark DataFrames\n",
    "\n",
    "Spark dataframes are organized data in rows and columns that are distributed between multiple computational cores. In other words, Spark DataFrames are very similar to Pandas dataframes except they are **distributed**. Spark dataframes are faster and more convenient to use compared to RDDs. \n",
    "\n",
    "In the next cell, we use `read` method followed by `toDF` method to load our favorite film's csv file as a Spark dataframe. Using the `option` method we indicate that our csv file has a header that should be skipped. Please note that we set column names for our new dataframe. The column names can be different than the header of the csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "films = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(\"supplementary/films.csv\").\\\n",
    "toDF('index','Title','Year','Length','Subject','Popularity')\n",
    "films.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## What's \"Parallel\" Here?\n",
    "\n",
    "In this segment, we will run SQL codes on virtual tables made from Spark DataFrames like `films` that we just created. But, a valid question is *what are we doing in parallel here?* <br>\n",
    "\n",
    "Let us know what you think below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "wk = widgets.Textarea(\n",
    "            value='',\n",
    "            placeholder='Write your answers here',\n",
    "            description='',\n",
    "            disabled=False,\n",
    "            layout=Layout( height='200px', min_height='100px', width='900px')\n",
    "            )\n",
    "\n",
    "def out1():\n",
    "    print('Submitted! See the answer in the next cell.')\n",
    "    \n",
    "display(wk)\n",
    "hourofci.SubmitBtn2(wk, out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Well, as you might have figured out, the point is that we are working with Spark DataFrames here, and we already know that they are \"distributed\" between multiple cores. So, whenever we execute a query, multiple cores will process our query in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Back to Our \"Life-Finder\" Example!\n",
    "\n",
    "In the previous segment we used `filter` method to retrieve all movies that have the word \"Life\" in their title from the film's RDD. Here, we want to write and execute actual SQL query to return the same result. \n",
    "\n",
    "But remember from the <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fintermediate-lessons%2Fgeospatial-data%2FWelcome.ipynb&branch=master\">intermediate lesson on Geospatial Data</a> to be able to execute SQL codes, we need relations (i.e., tables or views) that are different from dataframes. \n",
    "\n",
    "Spark SQL provides an environment to create SQL *Views* (i.e., virtual tables) from spark dataframes. We can do this using `createOrReplaceTempView` as follows. We name our View as `films_table`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "films.createOrReplaceTempView(\"films_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, time to write and execute a SQL query to fetch all movies containing the word \"Life\" in their title. The following SQL query is one way to do this. \n",
    "\n",
    "\n",
    ">```sql\n",
    "SELECT *\n",
    "FROM films_table f\n",
    "WHERE f.Title LIKE '%Life%'\n",
    "```\n",
    "\n",
    "In this query, we select all (`*`) rows from the films_table that is renamed to `f` that meet our condition in the `WHERE`-clause. The condition is that the title includes (`LIKE` function) the word \"Life\". The percentage (`%`) symbol means there could be any character comming before and after the word of interest. \n",
    "\n",
    "In the cell below, we execute this query using `sql` method and print the result using the `show()` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "film_result = spark.sql(\n",
    "            \"\"\"\n",
    "            SELECT *\n",
    "            FROM films_table f\n",
    "            WHERE f.Title LIKE '%Life%'\n",
    "            \"\"\")\n",
    "\n",
    "film_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### One More Example \n",
    "\n",
    "Cool! To practice this more, let's look at the next query. \n",
    "><b>Query:</b> Select all films that are shorter than 30 minutes and are highly popular (i.e., have popularity index higher than 50).\n",
    "\n",
    "<b>Solution:</b> \n",
    "And here is the SQL query that can address the query. \n",
    ">```sql\n",
    "SELECT *\n",
    "FROM films_table f\n",
    "WHERE f.length < 30 and f.popularity > 50```\n",
    "\n",
    "Let's execute it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "short_films = spark.sql(\n",
    "            \"\"\"\n",
    "            SELECT *\n",
    "            FROM films_table f\n",
    "            WHERE f.length < 30 and f.popularity > 50\n",
    "            \"\"\")\n",
    "\n",
    "short_films.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Converting Spark DataFrames to Pandas \n",
    "\n",
    "Once the query is executed using multiple cores, sometimes we want to convert the results to regular Pandas dataframes to use Pandas's functionalities. We can do this as simple as using `toPandas()` method as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "film_result_df = film_result.toPandas()\n",
    "film_result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Converting Pandas DataFrame to Spark DataFrame\n",
    "\n",
    "You might wonder what if we have a Pandas dataframe and we want to convert it to Spark DataFrame and try parallel computing?\n",
    "\n",
    "Well, Spark's `createDataFrame` function will do it. In the cell below we convert our `film_result_df` dataframe back to Spark dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(film_result_df).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## What About Spatial Datasets?\n",
    "So far so good, but what if we want to load a shapefile as a Spark dataframe? \n",
    "\n",
    "A very first thought would be loading the shapefile into a GeoDataFrame and then converting it to Spark dataframe, right? \n",
    "\n",
    "Let's try it then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gdf = gpd.read_file(\"supplementary/MN_points/POINT.shp\")\n",
    "  \n",
    "points = spark.createDataFrame(gdf) \n",
    "points.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Oops!!! There is something wrong here! Seems like Spark is failing to recognize the `geometry` data type for the `geometry` column.\n",
    "\n",
    ">Why do you think this problem occurred? <br>\n",
    "When do you think this will be problematic? <br>\n",
    "What solution do you propose to debug?\n",
    "\n",
    "Let us know in the textbox below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "w = widgets.Textarea(\n",
    "            value='',\n",
    "            placeholder='Write your answers here',\n",
    "            description='',\n",
    "            disabled=False,\n",
    "            layout=Layout( height='200px', min_height='100px', width='900px')\n",
    "            )\n",
    "\n",
    "def out1():\n",
    "    print('Submitted! See the answer in the next cell.')\n",
    "    \n",
    "display(w)\n",
    "hourofci.SubmitBtn2(w, out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Well, the problem is that we no longer have the `geometry` data type. After this conversion, the `geometry` column contains some garbled useless values. \n",
    "\n",
    "This is because PySpark **does not** support spatial data. Therefore, we need to install and use the spatial extension of Spark. It is called **Apache Sedona**.\n",
    "\n",
    "In the next segment, we will see how to handle spatial data with Apache Sedona. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" \n",
    "href=\"pc-6.ipynb\">Click here to go to the next notebook.</a></font>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='hciheader'></div><div class='hcifooter'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
