{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intermediate Parallel Computing\n",
    "### Segment 2 of 5\n",
    "\n",
    "### Apache Spark, the Spark of a Journey!\n",
    "\n",
    "### In this segment we will answer:\n",
    "* What is Apache Spark?\n",
    "* What is its architecture?\n",
    "* How to install and initiate a Spark computing session?\n",
    "\n",
    "\n",
    "*Lesson Developer: Mohsen Ahmadkhani, ahmad178@umn.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thank you for helping our study\n",
    "\n",
    "\n",
    "<a href=\"#/slide-1-0\" class=\"navigate-right\" style=\"background-color:blue;color:white;padding:8px;margin:2px;font-weight:bold;\">Continue with the lesson</a>\n",
    "\n",
    "Throughout this lesson you will see reminders, like the one below, to ensure that all participants understand that they are in a voluntary research study.\n",
    "\n",
    "### Reminder\n",
    "\n",
    "<font size=\"+1\">\n",
    "\n",
    "By continuing with this lesson you are granting your permission to take part in this research study for the Hour of Cyberinfrastructure: Developing Cyber Literacy for GIScience project. In this study, you will be learning about cyberinfrastructure and related concepts using a web-based platform that will take approximately one hour per lesson. Participation in this study is voluntary.\n",
    "\n",
    "Participants in this research must be 18 years or older. If you are under the age of 18 then please exit this webpage or navigate to another website such as the Hour of Code at https://hourofcode.com, which is designed for K-12 students.\n",
    "\n",
    "If you are not interested in participating please exit the browser or navigate to this website: http://www.umn.edu. Your participation is voluntary and you are free to stop the lesson at any time.\n",
    "\n",
    "For the full description please navigate to this website: <a href=\"../../gateway-lesson/gateway/gateway-1.ipynb\">Gateway Lesson Research Study Permission</a>.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Hide warnings\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "# HTML(''' \n",
    "#     <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "#     <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "# ''')\n",
    "\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <style>\n",
    "        .output_prompt{opacity:0;}\n",
    "    </style>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Apache Spark?\n",
    "\n",
    "Apache Spark is an open-source data processing framework designed to carry out processing tasks on large volumes of data. It enables dividing of data into smaller chuncks and distributing them between multiple processing units to perform data processing tasks at the same time. \n",
    "\n",
    "\n",
    "\n",
    "<!-- <img src=\"https://www.interviewbit.com/blog/wp-content/uploads/2022/06/Apache-Spark-1536x714.png?raw=1\" width=\"80%\"> -->\n",
    "<!-- <img src=\"https://blog.kakaocdn.net/dn/uO5KB/btqA2a0rio3/n95MXm1PSKxJzz1oaoCA21/img.png?raw=1\" width=\"50%\"> -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark architecture\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: #fff; text-align: left; vertical-align:\">\n",
    "        <td style=\"background: #fff; text-align: left; font-size: 23px;\">\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <b>Driver Program:</b> The code we write to process our data using the Apache Spark framework is indeed the driver program. In the driver program, the first thing we do is build a Spark Session.\n",
    "                </li>\n",
    "                <li>\n",
    "                    <b>Spark Session:</b> Spark Session is a gateway to all Spark functionalities (methods, data types, etc.). \n",
    "                </li>\n",
    "                <li>\n",
    "                    <b>Cluster Manager:</b> The cluster manager takes care of dividing the job into multiple smaller tasks and distributes them between worker nodes. \n",
    "                </li>\n",
    "                <li>\n",
    "                    <b>Worker Node:</b> A worker node or a slave node executes the tasks assigned by the cluster manager. Each worker node has one or more executors that can have multiple tasks to execute in parallel.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </td>\n",
    "        <td style=\"width: 50%; background: #fff; text-align: left; vertical-align: top;\"> \n",
    "            <img src=\"supplementary/spark_ar.png\" width=\"100%\">\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before jumping into the code...\n",
    "<table>\n",
    "    <tr style=\"background: #fff; text-align: left; vertical-align:\">\n",
    "        <td style=\"background: #fff; text-align: left; font-size: 23px;\">\n",
    "            <p>Apache Spark has a core engine and four extensions:</p>\n",
    "            <ul>\n",
    "                <li>\n",
    "<b>Spark SQL (enables executing SQL queries on data)</b>\n",
    "                </li>\n",
    "                <li>\n",
    "Spark Streaming (streaming data add-on)\n",
    "                </li>\n",
    "                <li>\n",
    "Spark MLlib (set of machine learning libraries)\n",
    "                </li>\n",
    "                <li>\n",
    "GraphX (designed for distributed graph processing Ex. LinkedIn, Facebook, etc).\n",
    "                </li>\n",
    "            </ul>\n",
    "</td>\n",
    "     <td style=\"width: 40%; background: #fff; text-align: left; vertical-align: top;\"> <img src='https://blog.kakaocdn.net/dn/uO5KB/btqA2a0rio3/n95MXm1PSKxJzz1oaoCA21/img.png?raw=1' width=\"500\" height=\"700\" alt='map'></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we will focus on **Spark SQL** only. Let's start by installing PySpark and then writing the driver program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, we need to install the Python version of Spark. It's called PySpark. <br>\n",
    "Run the cell below to install it using python package installer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, click the \"Restart Kernel\" to update the list of installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def restarter():\n",
    "    display(HTML(\n",
    "        '''\n",
    "            <script>\n",
    "                code_show = false;\n",
    "                function restart_kernel(){\n",
    "                    IPython.notebook.kernel.restart();\n",
    "                }\n",
    "            </script>\n",
    "            <button onclick=\"restart_kernel()\">Restart Kernel</button>\n",
    "        '''\n",
    "    ))\n",
    "restarter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Allright, now let's build a Spark Session. Remember, this is the first thing to do in the driver program. \n",
    "\n",
    "We use SparkConf() function to set a name to our application and specify the number of threads for our program to run. \n",
    "\n",
    "Here, we call it 'hourofci' and specify 4 **threads** in our local machine, then store the session in a variable named `spark`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName(\"hourofci\").setMaster(\"local[4]\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cool! Now we have a platform to perform our data analysis using 4 threads in parallel! \n",
    "\n",
    "Now, quiz time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "widget1 = widgets.RadioButtons(\n",
    "    options = ['No', 'Yes'],\n",
    "    description = 'Is the number of threads equivalent to the number of physical CPU cores in our machine?', style={'description_width': 'initial'},\n",
    "    layout = Layout(width='100%',display=\"flex\", justify_content=\"flex-start\"),\n",
    "    value = None\n",
    ")\n",
    "\n",
    "display(widget1)\n",
    "def out():\n",
    "    return print('They are not the same, actually!')\n",
    "\n",
    "hourofci.SubmitBtn2(widget1, out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallelising Using Threads!\n",
    "\n",
    "The number of threads is the number of gates you specify to process your data in parallel. It is usually specified as the number of available physical cores. We saw how to used `setMaster()` function to pass the variable `local` to specify the number of threads. The followings are the available parallelism options:\n",
    "\n",
    "* `local`: Run Spark with a single worker thread - No parallelism at all!\n",
    "\n",
    "* `local[N]`: Run Spark with N multiple worker threads.\n",
    "\n",
    "* `local[*]`: Set the number of worker threads as same as the number of available physical CPU cores on your machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Please note that we have two other oprions as `local[N,F]` and `local[*,F]` for specifying F maxFailures that we will not get into it's details in this lesson. See <a href=\"https://spark.apache.org/docs/latest/configuration.html\">here</a> for more information. \n",
    "\n",
    "If you are curious to see how many physical cores you have right now, run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print('The number of available physical CPU cores: ', os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great! Now you have a general sense that what Spark is and how to install and load it!<br> \n",
    "In the next segment we will get into some more details of **Spark SQL**.<br><br>\n",
    "\n",
    "\n",
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" href=\"pc-4.ipynb\">Click here to go to the next notebook.</a></font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='hciheader'></div><div class='hcifooter'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
