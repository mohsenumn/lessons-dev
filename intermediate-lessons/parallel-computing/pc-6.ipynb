{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intermediate Parallel Computing\n",
    "\n",
    "### Segment 5 of 6\n",
    "\n",
    "### PySpark SQL III: Spatial is Special!\n",
    "\n",
    "#### In this segment we will learn:\n",
    "* Apache Sedona\n",
    "* Querying spatial data with PySpark SQL\n",
    "* The benefits of parallelization for spatial data and analysis\n",
    "\n",
    "\n",
    "*Lesson Developer: Mohsen Ahmadkhani, ahmad178@umn.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "<a href=\"#/slide-2-0\" class=\"navigate-right\" style=\"background-color:blue;color:white;padding:8px;margin:2px;font-weight:bold;\">Continue with the lesson</a>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "<font size=\"+1\">\n",
    "\n",
    "By continuing with this lesson you are granting your permission to take part in this research study for the Hour of Cyberinfrastructure: Developing Cyber Literacy for GIScience project. In this study, you will be learning about cyberinfrastructure and related concepts using a web-based platform that will take approximately one hour per lesson. Participation in this study is voluntary.\n",
    "\n",
    "Participants in this research must be 18 years or older. If you are under the age of 18 then please exit this webpage or navigate to another website such as the Hour of Code at https://hourofcode.com, which is designed for K-12 students.\n",
    "\n",
    "If you are not interested in participating please exit the browser or navigate to this website: http://www.umn.edu. Your participation is voluntary and you are free to stop the lesson at any time.\n",
    "\n",
    "For the full description please navigate to this website: <a href=\"../../gateway-lesson/gateway/gateway-1.ipynb\">Gateway Lesson Research Study Permission</a>.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display, Latex, Markdown\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, VBox, Button\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "class Output:\n",
    "    def __init__(self, name='cmd_out'):\n",
    "        self.h = display(display_id=name)\n",
    "        self.content = ''\n",
    "        self.mime_type = None\n",
    "        self.dic_kind = {\n",
    "            'text': 'text/plain',\n",
    "            'markdown': 'text/markdown',\n",
    "            'html': 'text/html',\n",
    "        }\n",
    "        \n",
    "    def display(self):\n",
    "        self.h.display({'text/plain': ''}, raw=True)\n",
    "        \n",
    "    def _build_obj(self, content, kind, append, new_line):\n",
    "        self.mime_type = self.dic_kind.get(kind)\n",
    "        if not self.mime_type:\n",
    "            return content, False\n",
    "        if append:\n",
    "            sep = '\\n' if new_line else ''\n",
    "            self.content = self.content + sep + content\n",
    "        else:\n",
    "            self.content = content\n",
    "        return {self.mime_type: self.content}, True\n",
    "        \n",
    "    def update(self, content, kind=None, append=False, new_line=True):\n",
    "        obj, raw = self._build_obj(content, kind, append, new_line)\n",
    "        self.h.update(obj, raw=raw)\n",
    "        \n",
    "\n",
    "def cont(gpd_time, sedona_time):\n",
    "    if round(gpd_time/sedona_time, 2)>1:\n",
    "        res = f'''*** \\n <font style=\"background-color: #E3FFF5; font-size: large; color: black; font-weight: bold\">\n",
    "        Comparing the two execution times, for executing the intersection operation, GeoPandas took <font size=5 color='red'>{round(gpd_time, 3)}</font> seconds and \n",
    "        Apache Sedona did it for us in only <font size=5 color='red'>{round(sedona_time, 3)}</font> seconds. It means parallelizing this operation with Spark SQL made \n",
    "        the spatial operation <font size=5 color='red'>{round(gpd_time/sedona_time, 2)}</font> times <font size=5 color='green'>FASTER</font>.\n",
    "        \\n ***\n",
    "        '''\n",
    "    else:\n",
    "        res = f'''**\n",
    "        Comparing the two execution times, for executing the intersection operation GeoPandas took {round(gpd_time, 3)} seconds and \n",
    "        Apache Sedona did it for us in {round(sedona_time, 3)} seconds. Although we parallelized the operation using Spark framework, \n",
    "        it did slightly slower than GeoPandas. This is because of the trade-off between the partitioning cost and the load of the input data.** \n",
    "        '''\n",
    "    return res\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <style>\n",
    "        .output_prompt{opacity:0;}\n",
    "    </style>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preface\n",
    "### Why Distributed Spatial Computing?\n",
    "\n",
    "<center><img src=https://media.makeameme.org/created/why-even-bother-5c8eb2.jpg width=300></center>\n",
    "In recent years, spatial technology has evolved a lot and we now have BIG spatial data. Some examples of spatial big data include location-based services (e.g., Uber, Lyft, scooter ride companies), remote sensing data, spatial social networks' data like twitter and FaceBook, weather maps, transportation, and countless others. Handling such BIG loads of spatial data needs <b>faster</b> database management technologies, and parallel computing is faster!\n",
    "\n",
    "By the way, if you are curious about spatial big data, we talked about it in the <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fintermediate-lessons%2Fbig-data%2FWelcome.ipynb&branch=master\">intermediate Big Data</a> lesson. So, take a look if you have not already.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Sedona\n",
    "\n",
    "So far, we briefly saw how Apache Spark works, but like most data management technologies, Apache Spark first was developed for non-spatial data. Why? Well, because <b>spatial is special!</b> Due to nature of spatial data, it is much more complex than non-spatial data, and therefore it is harder to store and analyse. \n",
    "To support spatial data, Apache launched an extension to Spark named <b><a href=\"http://sedona.apache.org\">Apache Sedona</a></b>. \n",
    "\n",
    "Apache Sedona (formerly GeoSpark) is a powerful tool that extends RDDs to geospatial RDDs (aka SpatialRDD). In simple words Apache Sedona enables two preliminary things: \n",
    "<ol>\n",
    "    <li>\n",
    "Distributing geospatial data between multiple computational cores \n",
    "    </li>\n",
    "    <li>\n",
    "Spatial functions and queries in Spark SQL\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "In this segment we touch on Apache Sedona and see how it works faster than regular data processing frameworks (like GeoPandas) through a demo spatial problem. \n",
    "\n",
    "Please note that in this lesson, we use \"(spatial) Spark SQL\" and \"(Apache) Sedona\" interchangeably. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exit the Presentation Mode Again!\n",
    "\n",
    "It's CODE time again! <br>\n",
    "We'll start by installing Apache Sedona package.\n",
    "\n",
    "Click on the **X** in the upper left hand corner to exit RISE Presentation mode and see the regular Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pip install apache-sedona --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, click the \"Restart Kernel\" to update the list of installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "def restarter():\n",
    "    display(HTML(\n",
    "        '''\n",
    "            <script>\n",
    "                code_show = false;\n",
    "                function restart_kernel(){\n",
    "                    IPython.notebook.kernel.restart();\n",
    "                }\n",
    "            </script>\n",
    "            <button onclick=\"restart_kernel()\">Restart Kernel</button>\n",
    "        '''\n",
    "    ))\n",
    "restarter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we are going to import the packages we need including sedona, pyspark, geopandas (for data processing) and ipyleaflet (for mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "import geopandas as gpd\n",
    "from ipyleaflet import Map, GeoData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, we create a spatially enabled spark context using the imported Sedona sub-modules. Don't worry too much if it looks complicated! You can copy and paste this for your project :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"Spatial Spark Demo\").\\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "    config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.2.1-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\") .\\\n",
    "    getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Outline \n",
    "To illustrate how much faster Spark SQL can work through parallelization, we will compare its performance with the GeoPandas module. In this segment, we will execute a spatial operation between two sample spatial datasets to solve a demo spatial problem: \n",
    ">**Which rivers and lake centerlines intersect the states of Minnesota and Washington?** \n",
    " \n",
    "\n",
    "To find out the answer, we will take the following steps: \n",
    "><ol style='margin= 100px '>\n",
    "    <li>\n",
    "        Data Collection<br>\n",
    "        <ul>\n",
    "            <li>\n",
    "        Download and read the <i>US states</i> and world's <i>rivers</i> datasets as <i>GeoPandas</i> dataframes. <br><br>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        Spatial Operation With <b>GeoPandas</b> (Not Parallelized)\n",
    "        <ul>\n",
    "            <li>\n",
    "        Perform the spatial intersection operation using GeoPandas and record the execution time. <br><br>\n",
    "            </li>\n",
    "        </ul> \n",
    "    </li>\n",
    "    <li>\n",
    "        Spatial Operation With <b> Apache Sedona</b>  (Parallelized)\n",
    "        <ul>\n",
    "            <li>\n",
    "        Do the same operation with Apache Sedona and record the execution time. <br><br>\n",
    "            </li>\n",
    "        </ul> \n",
    "    </li>\n",
    "    <li>\n",
    "        Conclusion \n",
    "    </li>\n",
    "</ol>\n",
    "    \n",
    "Let's do it! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### World-Scale Rivers and Lake Centerlines Dataset\n",
    "This shapefile includes all the rivers and lake centerlines in the world with 10 meters accuracy. \n",
    "This is the same dataset we used <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fbeginner-lessons%2Fgeospatial-data%2Fgd-example_1.ipynb&branch=master\">here</a> in the beginner geospatial data lesson. You can learn more about this dataset there. \n",
    "\n",
    "Run the cell below to download the dataset as a zip file using the `wget` module and then extract the shapefile using `unzip`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!wget -O ne_10m_rivers_lake_centerlines.zip https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/physical/ne_10m_rivers_lake_centerlines.zip \n",
    "!unzip -n ne_10m_rivers_lake_centerlines.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's read in the shapefile using geopandas and take a look at the first few rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Use geopandas to read the file we just downloaded and unzipped through wget and unzip \n",
    "rivers = gpd.read_file('ne_10m_rivers_lake_centerlines.shp')\n",
    "# Look at the first few rows\n",
    "rivers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### US States Dataset\n",
    "\n",
    "Next, we will do the same for the `US states` shapefile. The `US states` shapefile contains the polygons of the US states. Run the following two cells to download it from the US Census government's <a href=\"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjBpayMwJn7AhV2lGoFHbfWB_cQFnoECAsQAQ&url=https%3A%2F%2Fwww.census.gov%2Fgeographies%2Fmapping-files%2Ftime-series%2Fgeo%2Fcarto-boundary-file.html&usg=AOvVaw2QKo7f-rChpkoO7zQ9E75A\">data warehouse</a> and read it as a GeoPandas Dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!wget -O us_states.zip https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_20m.zip \n",
    "!unzip -n us_states.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "states = gpd.read_file('us_states.zip')\n",
    "states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### An Important Caveat\n",
    "\n",
    "Before we start working with multiple spatial datasets we MUST make sure they are all in the same <a href='https://en.wikipedia.org/wiki/Spatial_reference_system'>coordinate reference system (CRS)</a>.\n",
    "\n",
    "We can pull the coordinate system information of each dataframe using the `.crs` attribute. Run the following two cells to see the coordinate systems information for our two dataframes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rivers.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "states.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Obviously, they are using different coordinate systems (WGS84 for rivers vs. NAD83 for states). We can use GeoPandas' `.to_crs(\"CRS ID\")` method to change the CRS of a dataframe. \n",
    "\n",
    "In the next cell, we change the CRS of the `states` dataframe to WGS84 by passing its CRS ID (4326) as an argument so both dataframes are at the same coordinate system. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "states = states.to_crs('4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Mapping and Visualization\n",
    "\n",
    "To visualize the loaded spatial datasets we use ipyleaflet package. In the next two cells, we create a data *layer*. Then we make a map using `Map` module setting a center point and an appropriate zoom level. These arguments vary according to the geographical extent of the datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# create an ipyleaflet layer for the rivers' dataset using GeoData module\n",
    "rivers_layer = GeoData(geo_dataframe = rivers, style={'color':'blue'})\n",
    "# create a map object and specify appropriate center point and zoom level according the extent of the data\n",
    "mymap1 = Map(center=(40,10), zoom = 2)\n",
    "# add the rivers' layer to the map to visualize\n",
    "mymap1.add_layer(rivers_layer)\n",
    "mymap1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "states_layer = GeoData(geo_dataframe = states, style={'color':'red'})\n",
    "mymap2 = Map(center=(40,-100), zoom = 4)\n",
    "mymap2.add_layer(states_layer)\n",
    "mymap2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 2. Spatial Operation With *GeoPandas* (Not Parallelized)\n",
    "\n",
    "Now that we have a better sense of our datasets by visualizing them, it's time to run the intersection operation using **GeoPandas**. To do this, we will first filter the two states of Minnesota and Washington using the following code.\n",
    "\n",
    "Note that here the character `|` is the logical `OR` operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "states[(states['NAME']=='Minnesota')|(states['NAME']=='Washington')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To operate the spatial function of `intersection` we will use GeoPandas `overlay` function setting its `how=` argument to `\"intersection\"`. \n",
    "\n",
    "We record the execution time using the `time` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "inter = gpd.overlay(rivers, states[(states['NAME']=='Minnesota')|(states['NAME']=='Washington')], how='intersection')\n",
    "\n",
    "gpd_time = time.time() - start_time\n",
    "print(f\"Execution time for GeoPandas: {gpd_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 3. Spatial Operation With *Apache Sedona* (Parallelized)\n",
    "\n",
    "\n",
    "### Converting GeoPandas to Apache Sedona\n",
    "\n",
    "To tackle this problem with Apache Sedona, first, we need to convert our input data to `spark dataframes`, something that Spark understands and can parallelize. We can perform this conversion for our GeoPandas dataframes using `createDataFrame` method provided by spark in the next cell. \n",
    "\n",
    "Here we also print the dataframe's schema to see what columns and data types we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "states_spdf = spark.createDataFrame(states)\n",
    "states_spdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that at the very bottom of the schema, there is a `geometry` data type! That's what Sedona brought to us. \n",
    "\n",
    "Next, we use the `show` method to see a few first rows of the spark dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "states_spdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And same process for the rivers' dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rivers_spdf = spark.createDataFrame(rivers)\n",
    "rivers_spdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Creating SQL Views (Virtual Tables)\n",
    "\n",
    "Similar to non-spatial data, we need to create SQL Views for each of the dataframes we have. This is a required step to enable querying data in SQL and means creating virtual relations (tables) for dataframes. Below, we do this using `createOrReplaceTempView` method and create two Views named `rivers_table` and `states_table`. We will query from these two tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rivers_spdf.createOrReplaceTempView(\"rivers_table\")\n",
    "states_spdf.createOrReplaceTempView(\"states_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Intersection Spatial Query \n",
    "\n",
    "Apache Sedona uses SQL language, so we need to write a spatial query.<br>\n",
    "In the following SQL query, we select state and river names along with the rivers' geometry. \n",
    "\n",
    "Under the `WHERE` clause, we set the conditions of *the state name being either Minnesota OR Washington* AND *the rivers intersect the polygons of these two states*. \n",
    "\n",
    "> ```sql\n",
    "SELECT states_table.NAME, rivers_table.name as river_name, rivers_table.geometry geom\n",
    "FROM states_table, rivers_table\n",
    "WHERE states_table.NAME IN ('Minnesota', 'Washington') and ST_INTERSECTS(rivers_table.geometry, states_table.geometry)\n",
    "\n",
    "\n",
    "\n",
    "Let's execute this spatial query together and record its execution time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "mn_rivers = spark.sql(\"\"\"\n",
    "SELECT states_table.NAME, rivers_table.name as river_name, rivers_table.geometry as geom\n",
    "FROM states_table, rivers_table\n",
    "WHERE states_table.NAME IN ('Minnesota', 'Washington') and ST_INTERSECTS(rivers_table.geometry, states_table.geometry)\n",
    "\"\"\")\n",
    "\n",
    "sedona_time = time.time() - start_time\n",
    "print(f\"Execution time for Sedona: {sedona_time} seconds. \\nReminder: GeoPandas did it in {gpd_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### What do you think?!\n",
    "<img src=\"supplementary/Pondering.jpg\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "display(Markdown('<b>Did you expect this result regarding the execution times?</b>'))\n",
    "o1='Yes'\n",
    "o2='No'\n",
    "widget1 = widgets.RadioButtons(\n",
    "    options = [o1, o2],\n",
    "    description = ' ', style={'description_width': 'initial'},\n",
    "    value = None\n",
    ")\n",
    "execute = Button(\n",
    "    description='Submit',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    ")\n",
    "\n",
    "children1 = [widget1, execute]\n",
    "vbox = VBox(children=children1)\n",
    "out = Output(name='cmd_out')\n",
    "def cmd(b):\n",
    "    print('Submitted!')\n",
    "    return out.update(Markdown(cont(gpd_time, sedona_time)))\n",
    "\n",
    "execute.on_click(cmd)\n",
    "display(vbox)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "out = Output(name='cmd_out')\n",
    "\n",
    "out.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Note\n",
    "Spatial queries like this one would take much longer if you do not partition them. Please note that this difference is more significant for larger datasets as the \"distribution\" of data between multiple worker nodes itself could be time-consuming. Hence, for small datasets we do not usually use parallel computing. In the exploration segment, we will explore this fact further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### [Optional] Converting Spark Dataframes Back to GeoPandas For Visualization\n",
    "\n",
    "Now to spatially visualize our query result we need to convert the result back to `geopandas dataframe`. To do this we simply use `toPandas` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mn_rivers_df = mn_rivers.toPandas()\n",
    "result = gpd.GeoDataFrame(mn_rivers_df, geometry=\"geom\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And here is the result on a map! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "wa_mn = states[(states['NAME']=='Minnesota') | (states['NAME']=='Washington')]\n",
    "wa_mn_layer = GeoData(geo_dataframe = wa_mn, style={'color':'red'})\n",
    "gdf_layer = GeoData(geo_dataframe = result, style={'color':'blue'})\n",
    "gdf_map = Map(center=(40,-100), zoom = 4)\n",
    "gdf_map.add_layer(gdf_layer)\n",
    "gdf_map.add_layer(wa_mn_layer)\n",
    "gdf_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Awesome! Now, click the link below to go to the exploration segment to dig in even more! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" \n",
    "href=\"pc-exploration.ipynb\">Click here to go to the next notebook.</a></font>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='hciheader'></div><div class='hcifooter'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
