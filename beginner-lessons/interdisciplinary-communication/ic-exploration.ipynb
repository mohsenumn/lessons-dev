{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interdisciplinary Communication\n",
    "### Segment 5 of 5\n",
    "# Exploration\n",
    "\n",
    "<i>Lesson Developer: </i>\n",
    "<ul>\n",
    "    <li>\n",
    "    <i>Forrest Bowlick (fbowlick@umass.edu)</i>\n",
    "    </li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interdisciplinary Communication Exploration\n",
    "\n",
    "In this final segment you will use cyberinfrastructure to computationally explore the words used in academic articles. This is a peek into computational linguistics and we'll use some natural language processing tools. If you want to know more about those things, google them!\n",
    "\n",
    "This segment is displayed in \"Notebook Mode\" rather than \"Presentation Mode.\" So you will need to scroll down to explore the content. Notebook mode allows you to see more content at once. It also allows you to easily compare and contrast cells and visualizations. \n",
    "\n",
    "Here you are free to explore as much as you want. Once you see how the code works, free to change attributes, code pieces, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "# Retreive the user agent string, it will be passed to the hourofci submit button\n",
    "agent_js = \"\"\"\n",
    "IPython.notebook.kernel.execute(\"user_agent = \" + \"'\" + navigator.userAgent + \"'\");\n",
    "\"\"\"\n",
    "Javascript(agent_js)\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "As always, you have to import the specific Python packages you'll need. However, for this exploration,since many of the functions are basic Python functions they don't require separate imports. So we'll import the packages as we need them so that you can see when we are using something more than the base functionality of Python. \n",
    "\n",
    "Remember to run each code cell by clicking the \"Run\" button to the left of the code cell. Wait for the <pre>In [ ]:</pre> to change from an asterisk to a number. That is when you know the code is finished running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What text to explore?\n",
    "As you'll see here, you can use the computer to deconstruct simple strings of letters into sentences, phrases and words. Words can be tagged with their \"parts of speech\"--nouns, verbs, prepositions, etc. That allows us to quantify and compare content, vocabulary, writing styles and sentiment, all things that make different disciplines unique in their forms of communication. The word clouds you worked with earlier in this lesson were created using these tools.\n",
    "\n",
    "Although the code in this exploration will work with any PDF in which the text is encoded (i.e. not just images of pages), to get started we'll work with some geospatial science journal articles. Later you can use any PDF file you wish. We'll start with a pair of articles published in a special 10 year anniversary issue of the open *Journal of Spatial Information Science* (http://JOSIS.org). Both articles discuss how spatial information science can help us examine mobility in transportation systems. This is a research area that often requires analysis of massive datasets varying over space and time (i.e. big data!). Good fodder for cyberinfrastructure and cyber literacy for GIScience.\n",
    "\n",
    "We'll compare these two short articles to see how similar they are: \n",
    "- Martin Raubal, 2020. <u>Spatial data science for sustainable mobility</u>. JOSIS 20, pp. 109–114, doi:10.5311/JOSIS.2020.20.651\n",
    "- Harvey J. Miller, 2020. <u>Movement analytics for sustainable mobility</u>. JOSIS 20, pp. 115–123 doi:10.5311/JOSIS.2020.20.663\n",
    "\n",
    "Since these are open source we can download them directly from their URLs:\n",
    "- Raubal - https://josis.org/index.php/josis/article/download/121/121/371\n",
    "- Miller - https://pdfs.semanticscholar.org/4102/402a6a1fb050b93a8c95e28589e5f0330b7c.pdf\n",
    "\n",
    "We'll start with Raubal's article, then you can try your hand at processing Miller's article. As much as possible, this code below is generic to make it easier to run again. You'll see a few times where we've created specially named result files to be sure we've still got them when we get around to comparing the two articles at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "First we get the document we want to examine from the web using the system command *!wget*. **wget** is a software package that helps us download files from the internet. We use it here to download our PDF files. Since you want to reuse this code later, we're going to put the downloaded document into a temporary file called *article.pdf*.\n",
    "\n",
    "Then we have to translate the PDF format into plain text using the system command *!pdftotext*, naming the output *article.txt*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://josis.org/index.php/josis/article/download/121/121/371 -O article.pdf --quiet\n",
    "!pdftotext -enc ASCII7 'article.pdf' 'article.txt'\n",
    "!echo Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to open the text file so that we can read it and begin our analysis. The following code opens *article.txt* for \"r\"eading, reads the file, saves the contents in the variable *article_string*, and then closes the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text = open(\"article.txt\", \"r\", encoding='utf-8') \n",
    "article_string = file_text.read()\n",
    "file_text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## View the data\n",
    "Once you have read the data, you should look at it to make sure it is what you expected before you start your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data\n",
    "You'll notice that there are a lot of \"typos\" in this translation from PDF to text. While we could, with some clever coding, get rid of the strange text, for now we'll just take off the top bit and the references, since they, in particular, are not very clean.\n",
    "\n",
    "We'll do this by finding the index (location) of the words \"Abstract\" and \"References\". Using these locations we will extract the text between these start and end words (see the [start:end] piece of code below). This is sometimes called 'trimming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = article_string.index('Abstract')\n",
    "end = article_string.index('References')\n",
    "article_extract = article_string[start:end]\n",
    "print(article_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fun Begins!\n",
    "Now, we introduce some advanced chunks of code that will enable text processing. We do not expect you to fully understand all of the details of the code, nor become an expert in this advanced technology. Just enjoy the exploration and see what makes sense to you. There are brief explanations of each code chunk that explain the key concepts involved. The code and the explanations contain technical jargon that are most likely unfamiliar to you. As you learn to communicate interdisciplinarily, it is important to reflect on these new experiences where you are exposed to unfamiliar concepts, words, and jargon. This is an opportunity to try out a new experience and hopefully pick up one or two ideas along the way. The most important aspect is to have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start processing\n",
    "To begin, we'll use a new module called TextBlob from the textblob package. Textblob is a Python package for processing textual data. For more information see the QuickStart guide: https://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "\n",
    "We have to import the module, then turn the trimmed text *article_extract* into a *textblob*, a specific data format used by TextBlob. Then we will be able to start analyzing this text using functions provided by the TextBlob module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tblob = TextBlob(article_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try some of the functions that can be applied to the textblob called *tblob*. Remember that to the computer this is just a string of characters representing text. To do textual analysis, we have to get the computer to recognize meaningful \"chunks\" of this text. Thus, we break the text string up into words, phrases and sentences.\n",
    "\n",
    "Here we will focus only on words since we'd need a cleaner version of the text for more complex analysis such as phrases. Run the next code and see what textblob comes up with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tblob.words\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty impressive when you realize the computer just started with a string of characters. Let's take this to the next level, by getting the computer to decide what kinds of words each of these are. This is called Part Of Speech (POS) tagging. For more information about POS tagging, see this Wikipedia article https://en.wikipedia.org/wiki/Part-of-speech_tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags = tblob.tags\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function produces a *list of tuples*. A **tuple** is an ordered collection of items (such as words) that cannnot change. A **list** is an ordered collection of items that can change. Here, each tuple contains a word and its associated tag (i.e. a pair of items). Can you figure out what the tags mean? \n",
    "\n",
    "[OK, here's the hint: NN are variations of nouns, NNP are proper nouns (names of people and places), VB are verbs, CC conjunctions, etc. ] \n",
    "\n",
    "Next we can quantify the words this author chose to use in their article. Let's focus only on nouns (NN and NNS, singular and plural). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_nouns = [word for (word, tag) in tags if tag == 'NN' or tag == 'NNS']\n",
    "\n",
    "#returns a Python list, as indicated by the square bracket at the start.\n",
    "article_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! But like any good programmer, you should scan this output to see if it looks good. \n",
    "\n",
    "Wait! There are lots of instances of ']' and 'https' in this supposed list of nouns. Let's quickly get rid of them so they don't contaminate our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nouns = {']', 'https'}  \n",
    "all_nouns = [noun for noun in article_nouns if noun not in non_nouns] \n",
    "all_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! There are still a few unique non-nouns, but we're only interested in the frequent nouns, so this is good.\n",
    "\n",
    "Now, before we go any further, let's keep a copy of these nouns for later.\n",
    "\n",
    "**IMPORTANT!**  Read the comment in the code chunk below that instructs you to change the variable name *Raubal_nouns* when you rerun the code as instructed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT! Change this variable name when you run your code again otherwise you will overwrite the file!\n",
    "Raubal_nouns = all_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, back to our processing. It's hard to see from this long list which nouns are the most frequently used. It is too much data. Let's see if we can get some information by visualizing this data in a word cloud. This is basically a data to information transformation through visualization, which is a lot of words to say that we are distilling a lot of data into a small amount of useful information in the form of a visual word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(colormap='cividis', background_color='white').generate(str(all_nouns))\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's interesting. Let's make a copy of that word cloud for later...\n",
    "\n",
    "**IMPORTANT!**  Again, pay attention to the important note to change the variable name when rerunning the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT! Change this variable name when you run the code again!\n",
    "Raubal_wordcloud = wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are a lot of words in this visualization and it's hard to see which words are, say, the top 25 most common words. Let's dig into this deeper with a bit of computation.\n",
    "\n",
    "We can start by counting up how many times each of these nouns was used. There are plenty of ways to do this frequency count. We'll use a **for loop,** which will iterate over each item in our list of article_nouns. We will store the results into a Python **dictionary**, a special kind of Python data format that stores unordered collection of items organized as pairs of keys and values. To learn more about dictionaries and loops check out the link [here](https://www.w3schools.com/python/python_dictionaries.asp). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create an empty dictionary, indicated by the curly brackets\n",
    "noun_count = {}\n",
    "\n",
    "# loop over the list of nouns, identifying unique words and incrementing\n",
    "# the total for repeated words\n",
    "for item in all_nouns:\n",
    "    if item in noun_count:     # We already saw this noun, so add 1\n",
    "        noun_count[item] += 1\n",
    "    else:                      # We have not seen this noun yet, so make the count 1\n",
    "        noun_count[item] = 1\n",
    "\n",
    "#show the resulting dictionary (note the curly brackets)\n",
    "noun_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Of course there are a lot of words used only once (after all that's good grammatical style), so let's create a new dictionary of only the top 25 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the code chunk below is deceptively simple, it's pretty sophisticated. See if this description makes sense to you. Deconstruct the code from the inside out. \n",
    "\n",
    "- We use a module from the package *operator* called **itemgetter**. Remember that dictionaries are composed of pairs of keys (also called items, here it's the nouns) and values (the counts). The module itemgetter will sequentially get the value for each item from our dictionary. \n",
    "\n",
    "- That result is sent to the **sorted** function to sort all of the items in our dictionary by value. \n",
    "\n",
    "- Then the result of the **sorted** function is transformed back to a dictionary using the **dict** function. This will create a *topN* dictionary containing the N most common words in our article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 25  #sets the number of items to extract\n",
    "from operator import itemgetter \n",
    "topN = dict(sorted(noun_count.items(), key = itemgetter(1), reverse = True)[:N])  \n",
    "topN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We've got a sorted dictionary of the top 25 nouns in the article. \n",
    "\n",
    "OK, let's save this result and then you can run the other article! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#IMPORTANT! Change this file name when you run again!\n",
    "Raubal_top25 = topN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the other article\n",
    "It's your turn to run the Miller article through the same analysis. Start at the beginning again, running each code cell, being sure to make the necessary changes to process the Miller article and not overwrite the key Raubal files. When you've generated 'Miller_nouns', 'Miller_wordcloud', and 'Miller_top25', you'll be ready for the next step.\n",
    "\n",
    "GO BACK TO THE TOP!\n",
    "_______________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the articles\n",
    "OK, now you've got two sets of files, Raubal's and Miller's. First let's look at the two wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Raubal_wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Miller_wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What differences and similarities do you see? \n",
    "\n",
    "Finally, let's look at the word counts in a table. For visualization purposes we can put the two top 25 lists into a single pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "Raubal_top25_df = pandas.DataFrame(list(Raubal_top25.items()),columns = ['noun','count']) \n",
    "Miller_top25_df = pandas.DataFrame(list(Miller_top25.items()),columns = ['noun','count']) \n",
    "result = pandas.concat([Raubal_top25_df, Miller_top25_df], axis=1).reindex(Raubal_top25_df.index)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these two lists tell you about differences in the articles? Can you spot some \"words\" that shouldn't be in this list? Oops, that would call for a bit more cleaning up, but for now, we're good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "**You have finished an Hour of CI!**\n",
    "\n",
    "If you would like a certificate, then please type your name below and click \"Create Certificate\" and you will be presented with a PDF certificate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# This code cell loads the Interact Textbox that will ask users for their name\n",
    "# Once they click \"Create Certificate\" then it will add their name to the certificate template\n",
    "# And present them a PDF certificate\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "def make_cert(learner_name, lesson_name):\n",
    "    cert_filename = 'hourofci_certificate.pdf'\n",
    "\n",
    "    img = Image.open(\"../../supplementary/hci-certificate-template.jpg\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    cert_font = ImageFont.load_default()\n",
    "    \n",
    "    cert_font   = ImageFont.truetype('../../supplementary/cruft.ttf', 150)\n",
    "    cert_fontsm = ImageFont.truetype('../../supplementary/cruft.ttf', 80) \n",
    "    _,_,w,h = cert_font.getbbox(learner_name)  \n",
    "    draw.text( xy = (1650-w/2,1100-h/2), text = learner_name, fill=(0,0,0),font=cert_font)\n",
    "    \n",
    "    _,_,w,h = cert_fontsm.getbbox(lesson_name)\n",
    "    draw.text( xy = (1650-w/2,1100-h/2 + 750), text = lesson_name, fill=(0,0,0),font=cert_fontsm)\n",
    "    img.save(cert_filename, \"PDF\", resolution=100.0)  \n",
    "    \n",
    "    return cert_filename\n",
    "\n",
    "\n",
    "interact_cert=interact.options(manual=True, manual_name=\"Create Certificate\")\n",
    "\n",
    "@interact_cert(name=\"Your Name\")\n",
    "def f(name):\n",
    "    print(\"Congratulations\",name)\n",
    "    filename = make_cert(name, 'Beginner Interdisciplinary Communication')\n",
    "    print(\"Download your certificate by clicking the link below.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" href=\"hourofci_certificate.pdf?download=1\" download=\"hourofci_certificate.pdf\">Download your certificate</a></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "I-GUIDE EWD",
   "language": "python",
   "name": "conda-env-iguide-ewd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
